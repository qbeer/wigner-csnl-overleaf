\documentclass[11pt, english]{article}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage[english]{babel}
\selectlanguage{english}
\usepackage[utf8]{inputenc}
\usepackage[svgnames]{xcolor}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{eucal}
\usepackage{amssymb}
\usepackage{mathrsfs}

\usepackage{listings}
\usepackage{afterpage}
\pagestyle{plain}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
language=Python,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
numbers=none,
keywordstyle=\color{blue},
numberstyle=\tiny\color{gray},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

\usepackage{here}


\textheight=21cm
\textwidth=17cm
\oddsidemargin=0cm
\pagestyle{plain}

\usepackage{color}
\usepackage{indentfirst}
\usepackage{ragged2e}

\global\let\date\relax
\newcounter{unomenos}
\setcounter{unomenos}{\number\year}
\addtocounter{unomenos}{-1}
\stepcounter{unomenos}
\gdef\@date{ \arabic{unomenos} }

\usepackage[backend=bibtex,style=phys]{biblatex}
\bibliography{references} 

\begin{document}

\begin{titlepage}

\begin{center}
Wigner Research Center for Physics, Hungarian Academy of Sciences - \@date\\
\vspace*{0.15in}
Computational Systems Neuroscience Lab \\
\vspace*{0.4in}
\rule{100mm}{0.1mm}\\
\vspace*{0.3in}
\begin{Large}
\textbf{Modelling of the visual cortex with \\ the methods of machine learning} \\
\end{Large}
\vspace*{0.3in}
\rule{100mm}{0.1mm}\\
\vspace*{0.4in}
\begin{large}
Alex Olar \\
\end{large}
\vfill
\includegraphics[width=2cm]{logoFAC.png}
\end{center}
\end{titlepage}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

\renewcommand{\thesection}{\Roman{section}.}
\renewcommand{\thesubsection}{\Roman{section}. \arabic{subsection}.}

\tableofcontents
\newpage

\section{Introduction}

\subsection{General}

\par  The visual cortex is a many layered unit in the brain, in which the first computational layer (V1) is known to detect edges in visual stimuli.
The second computational layer is doing further abstraction but we do not yet
have proper information about its role in vision \cite{ZiembaV2}. For the sake of acquiring a
better understanding of the visual representation of the brain, it is crucial to be able to build models that are on one hand not similar in every regard to the brain but on the other hand reproduce its properties, responses to visual input.
\par Since the brain is capable of behaving generatively, such as imagining things that do not exist from memory, therefore we should aim for developing such models. In this aspect, the latent representational models proved to be very good candidates as the actively researched variational autoencoders (VAE) from the domain of deep learning \cite{kingma2013auto}. This model attempts to represent high-dimensional images in a smaller vector space with auxiliary constraints on the latent representation using deep neural networks.

\subsection{Motivation}

\par The selection of this topic was motivated by the substantial development in neurobiology that could be achieved with the use and development of deep learning models. We could gather higher level information about the image-processing of the brain. It is important to mention that this research is on the border of different sciences such as information technology, neurobiology consequently it poses an exciting challenge to understand.

\section{Models}

\subsection{Early models}

\par PCA with sparse coding \cite{olshausen1996emergence} ... (understanding of V1)

\subsection{VAE}

\par Autoencoders are mostly used to reduce dimensionality of data in an unsupervised manner. They consist of an encoder and a decoder model surrounding the reduced representation. These models are usually realized as neural network due to their generalization capability. Autoencoders are a crucial tool for data compression and can reach better results than traditional tools \cite{theis2017lossy}. Variational autoencoders tackle the same problem while constraining the latent, reduced representation. This sacrifices reconstruction accuracy in order to learn a better representation of the dataset.

\subsubsection{Math}

\par Considering a dataset $\boldsymbol{X} = \big\{\boldsymbol{x}^{i}\big\}_{i = 1}^{N}$ that consists of independently and identically distributed samples of an underlying distribution it can be assumed that the data has some underlying, unobserved variable $\boldsymbol{z}$. The value $\boldsymbol{z}^{i}$, corresponding to a sample $\boldsymbol{x}^{i}$, is first generated from a prior distribution $p_{\boldsymbol{\theta_{real}}}(\boldsymbol{z})$ and $\boldsymbol{x}^{i}$ is then generated from a conditional distribution $p_{\theta_{real}}(\boldsymbol{x} | \boldsymbol{z})$. Since the real model parameters $\theta_{real}$ are unknown we can only approximate them with the encoder and decoder parametric models.
\par We want to approximate the posterior probability:

\begin{equation}
    p_{\boldsymbol{\theta}}(\boldsymbol{z} | \boldsymbol{x}) \propto p_{\boldsymbol{\theta}}(\boldsymbol{x} | \boldsymbol{z}) p_{\boldsymbol{\theta}}(\boldsymbol{z})
\end{equation}

\par Where basically we just applied Bayes-theorem for the likelihood and the prior to acquire the posterior. We build an encoder/recognition model $q_{\phi}(\boldsymbol{z} | \boldsymbol{x})$ and a decoder model $p_{\theta}(\boldsymbol{x} | \boldsymbol{z})$. The probabilistic encoder produces a distribution over $\boldsymbol{z}$ that should properly approximate the prior while the  ...

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{vae.png}
    \caption{Variational autoencoder}
    \label{fig:vae}
\end{figure}

\par The model was built in Keras \cite{chollet2015keras} and using its built-in visualization tool I was able to show how my VAE is built:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{vae_keras.png}
    \caption{Variational autoencoder implementation in Keras}
\end{figure}

\subsection{LVAE}

\par The ladder variational autoencoder was first introduced in 2016 \cite{sonderby2016ladder} one year after the UNET \cite{ronneberger2015u} architecture which used ladder like parameter sharing across encoding and decoding layers.

\par My implementation consists only dense layers and rectified linear units. Basically each unit of the two-unit ladder encoder includes an MLP. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{lvae.png}
    \caption{Ladder variational autoencoder}
\end{figure}

\par The model was built in Keras \cite{chollet2015keras} and using its built-in visualization tool I was able to show how my LVAE is built:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{lvae_keras.png}
    \caption{Ladder variational autoencoder implementation in Keras}
\end{figure}

\newpage
\printbibliography

\end{document}